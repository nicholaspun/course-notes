\subsection{Review of LP theory}
A \underline{linear program} (LP) is an optimization problem of the form:
\begin{equation}\label{eq:intro-review-lp}\tag{P}
\begin{aligned}
    \max \quad c^\intercal x & \\
    \text{s.t.} \quad Ax &\leq b \\
    x &\geq 0
\end{aligned}
\end{equation}
where $x \in \R^n, \ A \in M_{m \times n}(\R) $, and the objective function and constraints are linear. We must also require that:
\begin{itemize}
    \item There are a finite number of variables and constraints
    \item The inequalities are non-strict
\end{itemize}

Any LP has 3 possible outcomes:
\begin{enumerate}
    \item The LP is \underline{infeasible}
    \item The LP is \underline{unbounded}, i.e. We can achieve feasible solutions of arbitrarily ``good" objective value. (For (\ref{eq:intro-review-lp}), this means that $\forall v \in \R$ there exists a feasible solution $x$ s.t $c^\intercal x > v$)
    \item The LP has an \underline{optimal solution}. (For (\ref{eq:intro-review-lp}), this means there is a feasible solution $x^*$ such that $c^\intercal x^* \geq c^\intercal x \ \forall \ \text{feasible solutions} \ x$)
\end{enumerate}

\begin{theorem}{Fundamental Theorem of Linear Programming}{}
    There are only these $3$ possible outcomes
\end{theorem}

\begin{theorem}{}{}
    LPs can be solved efficiently
\end{theorem}

\subsubsection{Duality}
\underline{Question}: How do we prove bounds on the optimal value \underline{OR} justify that a solution is optimal?

\underline{Idea}: We can prove bounds by taking a suitable linear combination of constraints of the LP.

For example, we can multiply the constraints of (\ref{eq:intro-review-lp}) by some vector $y \geq 0$. This gives:
\begin{equation*}
    y^\intercal Ax \leq y^\intercal b \quad (y \geq 0)
\end{equation*}
Notice that if we require that $c^\intercal \leq y^\intercal A$, then since $x \geq 0$, we get the following chain of inequalities:
\begin{equation*}
    c^\intercal x \leq y^\intercal Ax \leq y^\intercal b 
\end{equation*}
And this is precisely the \underline{dual LP}.
\begin{equation}\label{eq:intro-review-lp-dual}\tag{D}
\begin{aligned}
    \min \quad b^\intercal y & \\
    \text{s.t.} \quad A^\intercal y &\geq c \\
    y &\geq 0
\end{aligned}
\end{equation}

We call the original LP the primal:
\begin{itemize}
    \item Every constraint of the primal is a variable in the dual
    \item Every variable of the dual is a constraint in the dual
\end{itemize}

\begin{remark}{}{}
    The dual of a dual gives us back the primal
\end{remark}

\subsubsection{Duality Theorems}
We'll use $(P)$ and $(D)$ to denote the primal and dual, respectively.

\underline{Weak Duality}:
If $x$ is feasible for $(P)$ and $y$ is feasible for $(D)$, then $c^\intercal x \leq b^\intercal y$

\begin{note}
    We can already infer things from this. For example, if $(D)$ is unbounded, then $c^\intercal x$ is unable to obtain any solution. So, it must be infeasible
\end{note}

\underline{Strong Duality}:
If $(P)$ has an optimal solution, then so does $(D)$.

Suppose $x^*$ is a feasible solution for $(P)$ and $y^*$ is a feasible solution for $(D)$. 

$x^*$, $y^*$ are optimal for $(P)$, $(D)$ respectively \underline{if and only if} $c^\intercal x = b^\intercal y$  \underline{if and only if}
\begin{itemize}
    \item $x^*_j \neq 0 \Rightarrow$ corresponding dual constraint is tight at $y^*$ (i.e. $(A^\intercal y^*)_j = c_j$)
    \item $y^*_i \neq 0 \Rightarrow$ corresponding primal constraint is tight at $y^*$ (i.e. $(Ax^*)_i = b_i$)
\end{itemize}
These two conditions are known as the complementary slackness (CS) conditions.

\subsection{Geometry of LPs}

A feasible region of an LP is called a \underline{polyhedron}, i.e. $P \subseteq \R^n$ is a polyhedron if it can be written as $\{x \in \R^n \::\: Ax \leq b \}$

A polyhedron is a \underline{convex set}. (A convex set is a a set $S \subseteq \R^n$ where $\forall x,y \in S, \forall \lambda \in [0,1], \lambda x + (1 - \lambda)y \in S$)

We say that $x \in \R^n$ is a convex combination of points $p^{(1)}, \ldots, p^{(k)} \in \R^k$ if $\exists \lambda_1, \ldots, \lambda_k \geq 0, \sum_{i = 1}^k = 1$, such that:
\begin{equation*}
    x = \sum_{i=1}^k \lambda_i p^{(i)}
\end{equation*}

An \underline{extreme point} of a convex set $S \subseteq \R^n$ is a point $\hat{x}$ such that $\hat{x}$ cannot be written as a convex combination of $2$ distinct points of $S$.

A polyhedron has a finite number (possibly zero) of extreme points.

$\hat{x}$ is an extreme point of a polyhedron $P \subseteq \R^n$ if and only if $\exists c \in \R^n$ such that $\hat{x}$ is a unique optimal solution to the LP: $\{\max c^\intercal x$ s.t. $x \in P\}$

\begin{theorem}{}{}
    Consider the (LP): $\{\max c^\intercal x$ s.t. $x \in P\}$ where $P \subseteq \R^n$ is a polyhedron. If (LP) has an optimal solution and $P$ has extreme points, then there is always an optimal solution that is an extreme point of $P$.
\end{theorem}

\begin{definition}{Convex Hull}{}
    Let $S \subseteq \R^n$, the \underline{convex hull} of $S$, denoted $conv(S)$ is the smallest convex set containing $S$. Equivalently:
    \begin{equation*}
        conv(S) := \{x \in \R^n \::\: x \text{ is a convex combination of a \underline{finite} number of points of } S\}
    \end{equation*}
\end{definition}

\begin{definition}{Polytope}{}
    A \underline{polytope} is a bounded polyhedron, i.e. $\exists \gamma \in \R$ such that $\forall x \in$ polytope, $|x_i| \leq \gamma \: \forall$ coordinates $i$.
\end{definition}

\begin{remark}~
    \begin{enumerate}
        \item A polytope is the convex hull of its extreme points
        \item $P \subseteq \R^n$ is a polytope if and only if $P = conv(S)$ for a \underline{finite} set $S \subset \R^n$
    \end{enumerate}
\end{remark}